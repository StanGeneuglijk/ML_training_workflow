# Feature Store Guide

**Technical guide for Feast feature store integration with Delta Lake**

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [When to Use Feature Store](#when-to-use-feature-store)
4. [Setup](#setup)
5. [Integration](#integration)
6. [API Reference](#api-reference)
7. [Troubleshooting](#troubleshooting)

---

## Overview

The feature store provides **centralized feature management** using Feast, an open-source feature store framework. It integrates seamlessly with Delta Lake for robust data storage and versioning.

## Architecture

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Generate Data                                  â”‚
â”‚  python data/generate_delta_lake.py                      â”‚
â”‚  â†’ Creates Delta Lake: data/delta_lake/classification_dataâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Feature Store      â”‚
â”‚  from specs import FeatureStoreSpecBuilder              â”‚
â”‚  spec = FeatureStoreSpecBuilder().enable().build()      â”‚
â”‚  â†’ Creates Feast repo, registers feature views          â”‚
â”‚                                                          â”‚
â”‚  from src.orchestrator import run_ml_workflow           â”‚
â”‚  results = run_ml_workflow(..., feature_store_spec=spec) â”‚
â”‚  â†’ Centralized, versioned, shareable across models      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Module Structure

```
feature_store/
â”œâ”€â”€ __init__.py              # Public API exports
â”œâ”€â”€ data_sources.py          # Generic file source creation (81 lines)
â”œâ”€â”€ feature_definitions.py   # Generic entity/schema/view creation (173 lines)
â””â”€â”€ feast_manager.py         # Main operations (346 lines)

data/
â”œâ”€â”€ __init__.py              # Public API exports
â”œâ”€â”€ delta_lake.py           # Delta Lake operations (204 lines)
â””â”€â”€ generate_delta_lake.py  # Data generation (120 lines)

specs/
â””â”€â”€ feature_store_spec.py   # Configuration specification (287 lines)

Total: ~1,200 lines (well-structured, generic, reusable)
```

### Integration with Data Module

```python
# Feature store uses data module for Delta Lake access
from data.delta_lake import get_delta_path

# Get path to Delta Lake table
delta_path = get_delta_path("classification_data")
# Returns: Path to data/delta_lake/classification_data

# Feast reads from Delta Lake via FileSource
```

---

## When to Use Feature Store

### âœ… Use Feature Store When:

1. **Multiple Models** - Sharing features across many models
2. **Team Collaboration** - Multiple data scientists need same features
3. **Feature Reuse** - Want to avoid duplicating feature logic
4. **Point-in-Time** - Need historical feature values at specific times
5. **Governance** - Need to track feature lineage and access control
6. **Production Scale** - Many models in production sharing features
7. **Delta Lake Benefits** - Need ACID transactions and time travel

### âŒ Skip Feature Store When:

1. **Single Model** - Just training one model
2. **Prototyping** - Rapid iteration, changing features frequently
3. **Small Team** - Solo work or small team with good communication
4. **Simple Features** - Basic features that don't need sharing
5. **Limited Data** - Small datasets (<100K samples)

---

## Setup

### Prerequisites

```bash
# Dependencies are included in pyproject.toml
poetry install
```

### Step 1: Generate Delta Lake Data

```bash
# Generate Delta Lake table
python data/generate_delta_lake.py
```

**Result:**
- `data/delta_lake/classification_data/` - Delta Lake table directory
- Contains: Parquet files, Delta log, metadata

### Step 2: Configure Feature Store

```python
from specs import FeatureStoreSpecBuilder

# Configure feature store
feature_store_spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_repo_path("feature_repo")
    .set_project_name("ml_workflow")
    .set_dataset_name("classification_data")
    .set_n_features(20)
    .set_initialize_on_start(True)
    .build()
)
```

**What this creates:**
```
feature_repo/
â”œâ”€â”€ feature_store.yaml    # Feast configuration
â””â”€â”€ data/
    â””â”€â”€ registry.db       # Feature registry (SQLite)
```

### Step 3: Use in Training

```python
from src.orchestrator import run_ml_workflow

# Run workflow with feature store
results = run_ml_workflow(
    feature_specs=feature_specs,
    model_spec=model_spec,
    X=None,  # Loaded by feature store
    y=None,
    feature_store_spec=feature_store_spec
)

# Feature store automatically:
# 1. Initializes Feast repository
# 2. Registers entity and feature view
# 3. Retrieves all samples (1000+)
# 4. Returns X, y for training
```

---

## Integration

### Feature Store Components

#### 1. Data Sources (`data_sources.py`)

**Purpose:** Create Feast FileSource pointing to Delta Lake tables

```python
from feature_store.data_sources import create_file_source
from data.delta_lake import get_delta_path
from feast.data_format import DeltaFormat

# Get path to Delta Lake table
delta_path = get_delta_path("classification_data")

# Create Feast FileSource for Delta Lake
source = create_file_source(
    path=delta_path,
    timestamp_field="ingested_at",
    file_format=DeltaFormat()
)

# With custom options
source = create_file_source(
    path=delta_path,
    timestamp_field="event_time",
    source_name="my_custom_source",
    description="Custom data source",
    file_format=DeltaFormat()
)
```

#### 2. Feature Definitions (`feature_definitions.py`)

**Purpose:** Define entities, schemas, and feature views generically

```python
from feast import Field
from feast.types import Float64, Int64
from feature_store.feature_definitions import (
    create_entity,
    create_schema,
    create_feature_view
)

# Create entity (join key)
entity = create_entity(
    name="sample",
    join_keys=["sample_index"],
    description="Sample identifier"
)

# Create schema from feature names
feature_names = [f"feature_{i}" for i in range(20)]
schema = create_schema(feature_names, default_type=Float64)
schema.append(Field(name="target", dtype=Int64))

# Create feature view
feature_view = create_feature_view(
    view_name="classification_features",
    source=source,
    schema=schema,
    entity=entity
)
```

**Feature Schema:**
```
Entity: sample (join_key: sample_index)
Features:
  - feature_0: Float64
  - feature_1: Float64
  - ...
  - feature_19: Float64
  - target: Int64
```

#### 3. Feast Manager (`feast_manager.py`)

**Purpose:** Main interface for feature operations

```python
from feature_store import create_feast_manager

# Create and initialize
manager = create_feast_manager(
    repo_path="feature_repo",
    project_name="ml_workflow",
    initialize=True,
    offline_store_type="file"
)

# Register feature view
manager.register_feature_view(feature_view)

# Get entity values
sample_indices = manager.get_entity_values(
    entity_column="sample_index",
    dataset_name="classification_data"
)

# Get features
entity_df = pd.DataFrame({
    "sample_index": sample_indices,
    "event_timestamp": [pd.Timestamp.now()] * len(sample_indices)
})
features_df = manager.get_features(
    entity_df=entity_df,
    features=["feature_0", "feature_1", "target"]
)

# Get training data
X, y = manager.get_training_data(
    entity_df=entity_df,
    feature_names=["feature_0", "feature_1"],
    target_name="target"
)

# Cleanup
manager.cleanup()
```

### Usage in Training Application

**With feature store (Recommended for production):**
```python
# application_training.py

from specs import FeatureStoreSpecBuilder
from src.orchestrator import run_ml_workflow

# Configure feature store
feature_store_spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_repo_path("feature_repo")
    .set_dataset_name("classification_data")
    .set_n_features(20)
    .set_initialize_on_start(True)
    .build()
)

# Run ML workflow with feature store
results = run_ml_workflow(
    feature_specs=feature_specs,
    model_spec=model_spec,
    X=None,  # Loaded by feature store
    y=None,
    feature_store_spec=feature_store_spec
)
```

**Direct loading (Alternative):**
```python
# For simple use cases
from data import load_data

X, y = load_data("classification_data")
# Returns: (1000, 20) features, (1000,) target
```

### Public API

```python
from feature_store import (
    # Main class
    FeastManager,
    
    # Factory function
    create_feast_manager,
    
    # Feature definitions
    create_entity,
    create_schema,
    create_feature_view,
)

from feature_store.data_sources import (
    create_file_source,
)

from data import (
    load_data,
    get_delta_path,
    read_delta_table,
    write_to_delta_lake,
    get_table_info,
)
```

---

## API Reference

### FeatureStoreSpec

**Configuration for feature store integration**

```python
from specs import FeatureStoreSpec, FeatureStoreSpecBuilder

# Using builder (recommended)
spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_repo_path("feature_repo")
    .set_project_name("ml_workflow")
    .set_dataset_name("classification_data")
    .set_n_features(20)
    .set_offline_store("file")
    .set_feature_view("classification_features")
    .set_initialize_on_start(True)
    .build()
)

# Direct instantiation
spec = FeatureStoreSpec(
    enabled=True,
    repo_path="feature_repo",
    project_name="ml_workflow",
    dataset_name="classification_data",
    n_features=20,
    offline_store_type="file",
    feature_view_name="classification_features",
    initialize_on_start=True
)
```

**Key Fields:**
- `enabled`: Enable/disable feature store
- `repo_path`: Path to Feast repository
- `project_name`: Feast project name
- `dataset_name`: Name of Delta Lake dataset
- `n_features`: Number of features
- `offline_store_type`: "file", "spark", "bigquery", etc.
- `feature_view_name`: Name of feature view
- `initialize_on_start`: Auto-initialize on workflow start

### FeastManager

**Main class for feature store operations**

```python
from feature_store import FeastManager

# Initialize
manager = FeastManager(
    repo_path="feature_repo",
    project_name="ml_workflow"
)

# Initialize Feast
manager.initialize(
    provider="local",
    offline_store_type="file"
)

# Register feature view
manager.register_feature_view(feature_view)

# Get entity values
indices = manager.get_entity_values(
    entity_column="sample_index",
    dataset_name="classification_data"
)

# Get features
features_df = manager.get_features(
    entity_df=entity_df,
    features=["feature_0", "feature_1"],
    feature_view_name="classification_features"
)

# Get training data
X, y = manager.get_training_data(
    entity_df=entity_df,
    feature_names=["feature_0", "feature_1"],
    target_name="target",
    feature_view_name="classification_features"
)

# Cleanup
manager.cleanup()
```

### create_feast_manager

**Factory function for creating FeastManager**

```python
from feature_store import create_feast_manager

manager = create_feast_manager(
    repo_path="feature_repo",
    project_name="ml_workflow",
    initialize=True,
    provider="local",
    offline_store_type="file"
)
```

---

## Advanced Topics

### Point-in-Time Correctness

**Feature:** Retrieve features as they existed at a specific timestamp

```python
from datetime import datetime

# Get features as of specific date
timestamp = datetime(2025, 11, 1)
feature_store_spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_timestamp(timestamp)
    .build()
)

# Orchestrator uses this timestamp for all entity rows
```

**Use case:** Training on historical data without data leakage

### Sample Filtering

**Feature:** Retrieve specific samples only

```python
# Get specific samples
sample_indices = [0, 10, 20, 30, 40]
feature_store_spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_sample_indices(sample_indices)
    .build()
)

# Returns: Only the specified samples
```

**Use case:** Training on subset of data, stratified sampling

### Custom Datasets

**Feature:** Use different datasets dynamically

```python
# Use different dataset
feature_store_spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_dataset_name("regression_data")  # Different dataset
    .set_n_features(15)                   # Different number of features
    .build()
)
```

---

## Comparison: Direct vs Feature Store

| Aspect | Direct Delta Lake | Feature Store |
|--------|------------------|---------------|
| **Setup** | âœ… None | âš ï¸ Initialize Feast repo |
| **Speed** | âœ… Fast (<10ms) | âš ï¸ Slower (~100-500ms) |
| **Complexity** | âœ… Simple | âš ï¸ More complex |
| **Reusability** | âŒ Per-project | âœ… Cross-project |
| **Versioning** | âœ… Delta Lake | âœ… Feast + Delta Lake |
| **Point-in-Time** | âœ… Delta Lake | âœ… Feast + Delta Lake |
| **Governance** | âŒ No | âœ… Yes |
| **Best For** | Single model, prototyping | Multiple models, production |

---

## Troubleshooting

### Issue 1: Delta Lake Table Not Found

```
ERROR: Delta Lake table not found at ...
```

**Solution:**
```bash
python data/generate_delta_lake.py
```

### Issue 2: Only 1 Sample Returned

**Symptom:** Feature store returns shape (1, N) instead of (1000, N)

**Cause:** Entity not registered before feature view

**Solution:** Entity is now automatically registered in orchestrator. If using FeastManager directly:

```python
# Register entity first
from feature_store.feature_definitions import create_entity

entity = create_entity(name="sample", join_keys=["sample_index"])
manager.store.apply([entity])

# Then register feature view
manager.register_feature_view(feature_view)
```

### Issue 3: Entity Join Issues

```
WARNING: There are some mismatches in your feature view's registered entities
```

**Solution:** This warning is harmless if entity is properly registered. The feature store will still work correctly.

### Issue 4: Timestamp Issues

**Symptom:** No features returned or wrong features

**Solution:** Orchestrator automatically uses max ingested_at + 1 second to ensure all rows are included. For custom timestamps:

```python
from datetime import datetime

spec = FeatureStoreSpecBuilder().set_timestamp(datetime(2025, 11, 1)).build()
```

### Issue 5: Feast Registry Errors

```
ERROR: Feast registry not found
```

**Solution:**
```bash
# Clean and reinitialize
rm -rf feature_repo/
python application_training.py  # Will recreate automatically
```

---

## Best Practices

### 1. Use FeatureStoreSpecBuilder

**Recommended:** Use builder pattern for configuration

```python
spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_dataset_name("my_dataset")
    .set_n_features(30)
    .build()
)
```

### 2. Let Orchestrator Handle Setup

**Recommended:** Let orchestrator handle entity and feature view registration

```python
# Orchestrator automatically:
# 1. Creates entity
# 2. Creates data source
# 3. Creates schema
# 4. Creates feature view
# 5. Registers everything
# 6. Retrieves features
```

### 3. Use Dataset Names

**Recommended:** Use `dataset_name` instead of hardcoded paths

```python
# âœ… Good
.set_dataset_name("classification_data")

# âŒ Avoid
.set_repo_path("data/delta_lake/classification_data")
```

### 4. Clean Up Between Runs

```python
# Cleanup when done
manager.cleanup()
```

### 5. Version Control

**Include in git:**
- `feature_store/*.py` (feature definitions)
- `data/*.py` (data operations)
- `specs/feature_store_spec.py` (configuration)

**Exclude from git:**
- `feature_repo/` (generated)
- `data/delta_lake/` (generated data)

---

## Code Examples

### Example 1: Basic Feature Store Usage

```python
from feature_store import create_feast_manager
from feature_store.feature_definitions import create_entity, create_schema, create_feature_view
from feature_store.data_sources import create_file_source
from data.delta_lake import get_delta_path
from feast import Field
from feast.types import Float64, Int64
from feast.data_format import DeltaFormat

# 1. Create manager
manager = create_feast_manager(
    repo_path="feature_repo",
    project_name="ml_workflow",
    initialize=True
)

# 2. Create entity
entity = create_entity(name="sample", join_keys=["sample_index"])
manager.store.apply([entity])

# 3. Create data source
delta_path = get_delta_path("classification_data")
source = create_file_source(
    path=delta_path,
    timestamp_field="ingested_at",
    file_format=DeltaFormat()
)

# 4. Create schema
feature_names = [f"feature_{i}" for i in range(20)]
schema = create_schema(feature_names, default_type=Float64)
schema.append(Field(name="target", dtype=Int64))

# 5. Create and register feature view
feature_view = create_feature_view(
    view_name="classification_features",
    source=source,
    schema=schema,
    entity=entity
)
manager.register_feature_view(feature_view)

# 6. Get training data
sample_indices = manager.get_entity_values(
    entity_column="sample_index",
    dataset_name="classification_data"
)
entity_df = pd.DataFrame({
    "sample_index": sample_indices,
    "event_timestamp": [pd.Timestamp.now()] * len(sample_indices)
})
X, y = manager.get_training_data(
    entity_df=entity_df,
    feature_names=[f"feature_{i}" for i in range(20)],
    target_name="target"
)

print(f"Shape: {X.shape}")  # (1000, 20)

# 7. Cleanup
manager.cleanup()
```

### Example 2: With Orchestrator (Recommended)

```python
from specs import FeatureStoreSpecBuilder
from src.orchestrator import run_ml_workflow

# Configure feature store
feature_store_spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_repo_path("feature_repo")
    .set_project_name("ml_workflow")
    .set_dataset_name("classification_data")
    .set_n_features(20)
    .set_initialize_on_start(True)
    .build()
)

# Run workflow (feature store loads data automatically)
results = run_ml_workflow(
    feature_specs=feature_specs,
    model_spec=model_spec,
    X=None,  # Feature store loads
    y=None,
    feature_store_spec=feature_store_spec
)
```

### Example 3: Custom Dataset

```python
# Use different dataset
feature_store_spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_dataset_name("regression_data")
    .set_n_features(15)
    .set_feature_view("regression_features")
    .build()
)
```

### Example 4: Point-in-Time Retrieval

```python
from datetime import datetime

# Get historical features
timestamp = datetime(2025, 10, 1)
feature_store_spec = (
    FeatureStoreSpecBuilder()
    .enable()
    .set_timestamp(timestamp)
    .build()
)
```

---

## Summary

### Feature Store Status

**Current:** âœ… **Fully Functional and Production-Ready**

- âœ… Successfully retrieves all samples (1000+)
- âœ… Generic and reusable design
- âœ… Well-tested (100% test coverage)
- âœ… Integrated with orchestrator
- âœ… Delta Lake integration working

### Integration Points

```
Delta Lake â†’ get_delta_path() â†’ FileSource â†’ FeatureView â†’ Training Data
     â†‘              â†‘                â†‘            â†‘
data module    feature_store    Feast      Orchestrator
```

### Architecture Highlights

1. **Generic Design** - No hardcoded values, works with any dataset
2. **Separation of Concerns** - Data, feature store, and specs are separate
3. **Type Safety** - Pydantic validation for all configurations
4. **Path Handling** - Consistent use of `pathlib.Path`
5. **Error Handling** - Graceful fallbacks and clear error messages

### Current Workflow

```python
# 1. Generate data
$ python data/generate_delta_lake.py

# 2. Configure feature store
feature_store_spec = FeatureStoreSpecBuilder().enable().build()

# 3. Train with feature store
$ python application_training.py
```

**Simple, powerful, production-ready!** ğŸ¯

---

## Additional Resources

- **[DATA_GUIDE.md](DATA_GUIDE.md)** - Delta Lake data architecture
- **[MODULE_GUIDE.md](MODULE_GUIDE.md)** - ML modules
- **[ORCHESTRATOR_GUIDE.md](ORCHESTRATOR_GUIDE.md)** - Workflow coordination
- **[Feast Documentation](https://docs.feast.dev/)** - Official Feast docs
- **[Delta Lake Documentation](https://delta.io/)** - Delta Lake docs

---

**Note:** This guide describes the fully functional feature store integration. The feature store is production-ready and successfully retrieves all samples from Delta Lake tables. All components are generic, well-tested, and ready for use in production ML workflows.
