version: '3.8'

services:
  # MLflow Tracking Server
  mlflow:
    build: .
    container_name: mlflow-server
    environment:
      - MLFLOW_TRACKING_URI=file:///app/mlruns
    volumes:
      - ./mlruns:/app/mlruns
      - ./data:/app/data
    ports:
      - "5000:5000"
    networks:
      - ml-network
    command: mlflow ui --host 0.0.0.0 --port 5000 --backend-store-uri file:///app/mlruns
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Training Application
  ml-training:
    build: .
    container_name: ml-training
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
      - ./data:/app/data
      - ./mlruns:/app/mlruns
      - ./logs:/app/logs
    networks:
      - ml-network
    depends_on:
      mlflow:
        condition: service_healthy
    command: python application_training.py

  # Inference API Server
  ml-serving:
    build: .
    container_name: ml-serving
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
      - ./mlruns:/app/mlruns
    ports:
      - "8000:8000"
    networks:
      - ml-network
    depends_on:
      mlflow:
        condition: service_healthy
    command: >
      python application_inference_serving.py serve
      --model-name gradient_boosting_classifier_registered
      --model-stage Production
      --host 0.0.0.0
      --port 8000

networks:
  ml-network:
    driver: bridge
